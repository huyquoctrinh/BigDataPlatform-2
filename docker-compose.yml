version: '3.8'

# x-airflow-common:
#   &airflow-common
#   image: apache/airflow:2.7.1
#   environment:
#     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
#     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://myuser:mypassword@postgres:5432/mydb
#     - AIRFLOW__CORE__LOAD_EXAMPLES=False
#     - AIRFLOW__CORE__LOGGING_LEVEL=INFO
#   volumes:
#     - ./orchestration/dags:/opt/airflow/dags
#     - ./orchestration/logs:/opt/airflow/logs
#     - ./etl:/opt/airflow/plugins
#     - ./orchestration/airflow.cfg:/opt/airflow/airflow.cfg
#     - ./config.ini:/opt/airflow/plugins
#   depends_on:
#     postgres:
#       condition: service_healthy  
#   networks:
#     - mynetwork

services:
  redis:
    restart: always
    image: redis:latest
    container_name: redis
    networks:
      - gateway
    ports:
      - "6379:6379"
    volumes:
      - ~/redis/data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 2s
      retries: 10

  kafka:
    restart: always
    image: confluentinc/cp-kafka:6.2.0
    container_name: kafka
    networks:
      - gateway
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      KAFKA_ADVERTISED_HOST_NAME: kafka:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

  kafdrop:
    restart: always
    image: obsidiandynamics/kafdrop:3.27.0
    networks:
      - gateway
    depends_on:
      - kafka
      - zookeeper
    ports:
      - 19000:9000
    environment:
      KAFKA_BROKERCONNECT: kafka:29092

  zookeeper:
    restart: always
    image: confluentinc/cp-zookeeper:6.2.0
    container_name: zookeeper
    networks:
      - gateway
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
  minio:
    restart: always
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: admin123
    volumes:
      - ~/minio/data:/data
    command: server /data --console-address ":9001"
    networks:
      - gateway
  
  # consumer:
  #   container_name: consumer
  #   build:
  #     context: ./build_scripts/kafka
  #     dockerfile: Dockerfile
  #   ports:
  #     - 8001:8001
  #   restart: "always"
  #   depends_on:
  #     - kafka
  #     - zookeeper
  #   networks:
  #     - gateway
  #   command: python3 /app/kafka_consumer.py
  #   volumes:
  #     - ./src/consumer:/app
  #     - ./src/consumer/logs:/app/logs
  mongodb_container:
    image: mongo
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - ~/mongodb/data:/data/db
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
    networks:
      - gateway

  # airflow-init:
  #   image: apache/airflow:2.7.1
  #   container_name: airflow_init
  #   entrypoint: /bin/bash
  #   command:
  #     - -c
  #     - airflow db init && (
  #       airflow users create
  #         --role Admin
  #         --username airflow
  #         --password airflow
  #         --email airflow@airflow.com
  #         --firstname airflow
  #         --lastname airflow )
  #   restart: on-failure

  #   networks:
  #     - gateway
  # airflow-webserver:
  #   image: apache/airflow:2.7.1
  #   command: airflow webserver
  #   ports:
  #     - "8081:8080"
  #   container_name: airflow_webserver
  #   restart: always 
  #   networks:
  #     - gateway

  # airflow-scheduler:
  #   image: apache/airflow:2.7.1
  #   command: airflow scheduler
  #   container_name: airflow_scheduler
  #   restart: always
  #   networks:
  #     - gateway
  worker:
    restart: "always"
    build:
      context: ./build_scripts/worker_process
      dockerfile: Dockerfile
    command: celery -A tasks worker --loglevel=DEBUG -E --logfile=/app/worker_logs/celery.log
    environment:
      CELERY_BROKER_URL: redis://redis
      CELERY_RESULT_BACKEND: redis://redis
    depends_on:
      - redis
      - zookeeper
      - kafka
    networks:
      - gateway
    volumes: 
      - ./src:/app
      - ./worker_logs:/app/worker_logs
      - shared-volumes:/tmp/
  app:
    # restart: "always"
    build:
      context: ./build_scripts/app_build
      dockerfile: Dockerfile
    container_name: app
    ports:
      - "5000:5000"
    volumes:
      - ./src:/app
      - ./logs:/app/logs
      - ./tmp_data:/app/tmp_data
      - shared-volumes:/app/tenants_data
    depends_on:
      redis:
        condition: service_healthy
    command: python3 /app/app.py
    environment:
      CELERY_BROKER_URL: redis://redis
      CELERY_RESULT_BACKEND: redis://redis
    networks:
      - gateway

networks:
  gateway:
    driver: bridge

volumes:
  shared-volumes:
    driver: local